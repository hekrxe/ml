# 决策树（Decision Tree）

决策树是一种通过递归分裂数据集来构建树状结构的监督学习算法，核心思想是**选择最优特征进行分裂，使数据集纯度最大化**。

---

## 一、核心概念

### 1.1 熵（Entropy）

熵用于衡量数据集的不确定性（混乱程度）：

$$
Ent(D) = -\sum_{k=1}^{|y|} p_k \log_2 p_k
$$

### 1.2 信息增益（Information Gain）

信息增益表示使用特征 $a$ 进行分裂后，数据集不确定性减少的程度：

$$
Gain(D, a) = Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} Ent(D^v)
$$

### 1.3 符号说明

| 符号 | 含义 |
|:---|:---|
| $Ent(D)$ | 数据集 $D$ 的熵（不确定性） |
| $\|y\|$ | 类别数量 |
| $p_k$ | 第 $k$ 类样本在 $D$ 中的比例 |
| $Gain(D, a)$ | 属性 $a$ 对数据集 $D$ 的信息增益 |
| $V$ | 属性 $a$ 的取值个数 |
| $D^v$ | 数据集 $D$ 中属性 $a$ 取值为 $v$ 的样本子集 |
| $\|D\|$ | 数据集 $D$ 的总样本数量 |
| $\|D^v\|$ | 子集 $D^v$ 的样本数量 |

---

## 二、算法步骤

利用信息熵原理选择信息增益最大的属性作为分类属性，递归地拓展决策树的分枝，完成决策树的构造。

### 2.1 整体流程

```
开始
  │
  ▼
检查停止条件 ──Yes──► 创建叶节点，返回
  │
  No
  │
  ▼
选择最优特征（信息增益最大）
  │
  ▼
根据最优特征分裂数据集
  │
  ▼
对每个子节点递归调用算法
  │
  ▼
返回构建好的子树
  │
  ▼
结束
```

### 2.2 详细步骤

1. **遍历所有特征**，计算每个特征的信息增益，选择最优特征
2. **根据最优特征分裂数据集**（按最佳分裂点），将数据集划分为多个子集
3. **对每个子集递归地应用以上步骤**，直到满足停止条件（如所有样本属于同一类别或特征用完）
4. **剪枝优化**（可选）：
   - **预剪枝**：在构建过程中提前停止
     - 限制树的最大深度
     - 限制叶节点的最小样本数
     - 限制信息增益的最小阈值
   - **后剪枝**：构建完成后自底向上剪枝
     - 检查每个内部节点
     - 如果剪枝后验证集准确率不降低，则剪枝

---

## 三、计算示例（鸢尾花数据集）

### 3.1 数据示例

| 花萼长度 | 花萼宽度 | 花瓣长度 | 花瓣宽度 | target |
|:---|:---|:---|:---|:---|
| 4.9 | 3.0 | 1.4 | 0.2 | setosa |
| 7.0 | 3.2 | 4.7 | 1.4 | versicolor |
| 6.3 | 3.3 | 6.0 | 2.5 | virginica |

数据集有 150 个样本：
- setosa: 50 个样本
- versicolor: 50 个样本
- virginica: 50 个样本

### 3.2 计算整体信息熵

计算每个类别的概率：

$$
\begin{aligned}
p_{\text{setosa}} &= \frac{50}{150} = \frac{1}{3} \\
p_{\text{versicolor}} &= \frac{50}{150} = \frac{1}{3} \\
p_{\text{virginica}} &= \frac{50}{150} = \frac{1}{3}
\end{aligned}
$$

计算根节点的信息熵：

$$
\begin{aligned}
Ent(D) &= -\sum_{k=1}^{3} p_k \log_2 p_k \\
&= -\left( \frac{1}{3} \log_2 \frac{1}{3} + \frac{1}{3} \log_2 \frac{1}{3} + \frac{1}{3} \log_2 \frac{1}{3} \right) \\
&= -3 \times \left( \frac{1}{3} \times (-1.585) \right) \\
&= 1.585
\end{aligned}
$$

### 3.3 信息增益的计算

#### 步骤

1. **选择特征**：遍历所有特征
2. **确定分裂点**：对每个特征，找到**最佳分裂点**
3. **计算子节点熵**：对每个分裂后的子集计算熵
4. **计算信息增益**：用父节点熵减去加权平均子节点熵

#### 按花瓣长度 <= 2.45 分裂数据集

这个分裂点（2.45）是决策树算法通过遍历所有可能的候选分裂点，并选择信息增益最大的那个点得到的。

**分裂后的分布**：

| 分裂条件 | setosa | versicolor | virginica | 总计 |
|:---|:---|:---|:---|:---|
| 花瓣长度 <= 2.45 | 50 | 0 | 0 | 50 |
| 花瓣长度 > 2.45 | 0 | 50 | 50 | 100 |

##### 左子树（花瓣长度 <= 2.45）

- 样本数：50，全为 setosa
- 概率：$p_{\text{setosa}} = 1$，其他类别概率为 0
- 熵：

$$
Ent(D^1) = - (1 \times \log_2 1) = 0
$$

##### 右子树（花瓣长度 > 2.45）

- 样本数：100，versicolor 和 virginica 各 50 个
- 概率：$p_{\text{versicolor}} = \frac{50}{100} = 0.5$，$p_{\text{virginica}} = 0.5$
- 熵：

$$
\begin{aligned}
Ent(D^2) &= -\left( 0.5\log_2 0.5 + 0.5\log_2 0.5 \right) \\
&= -2 \times (0.5 \times (-1)) \\
&= 1.0
\end{aligned}
$$

##### 计算信息增益

$$
Gain(D, a) = Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} Ent(D^v)
$$

1. **计算加权平均子节点熵**：
   - 左子节点权重：$\frac{50}{150} = \frac{1}{3}$
   - 右子节点权重：$\frac{100}{150} = \frac{2}{3}$
   - 加权平均：$\frac{1}{3} \times 0 + \frac{2}{3} \times 1.0 = \frac{2}{3} \approx 0.667$

2. **计算信息增益**：

$$
Gain = 1.585 - 0.667 = 0.918
$$

如此就得到信息增益值 0.918，且是所有分裂点中的最大值，因此决策树算法会选择花瓣长度 <= 2.45 作为第一个分裂点。

---

## 四、决策树可视化

![决策树可视化](img/decision_tree_iris.png)

---

## 五、关键参数

| 参数 | 作用 | 推荐值 |
|:---|:---|:---|
| `criterion` | 分裂标准 | `"entropy"` 或 `"gini"` |
| `max_depth` | 最大深度 | 3-10 |
| `min_samples_split` | 分裂所需最小样本数 | 2-20 |
| `min_samples_leaf` | 叶节点最小样本数 | 1-10 |
| `max_features` | 考虑的最大特征数 | `"sqrt"` 或 `"log2"` |

---

## 六、总结

| 步骤 | 核心操作 | 目标 |
|:---|:---|:---|
| 1 | 检查停止条件 | 判断是否继续分裂 |
| 2 | 选择最优特征 | 信息增益最大化 |
| 3 | 分裂数据集 | 提高子节点纯度 |
| 4 | 递归构建子树 | 完成树的构建 |
| 5 | 剪枝优化 | 防止过拟合 |

**核心思想**：通过**递归分裂**和**信息增益最大化**，构建一棵能够准确分类的决策树，同时通过**剪枝**防止过拟合。
